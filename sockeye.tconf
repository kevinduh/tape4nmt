# Define the data sets and language pair.
# Choose one of the following files, or simply paste the contents here.

# import "confs/wmt18-de-en.tconf"
# import "confs/wmt18-en-de.tconf"
# import "confs/wmt18-ru-en.tconf"
# import "confs/wmt18-en-ru.tconf"

global {

  SRC=(Direction: ruen="ru" enru="en")
  TRG=(Direction: ruen="en" enru="ru")

  train_prefix="/home/shuoyangd/projects/whale19_masking/mexps/data/train.100000"
  dev_prefix="sacrebleu://wmt16 sacrebleu://wmt17"
  test_prefix="sacrebleu://wmt18"
}

global {
  ##################################################################################################
  # General options you should set for your environment
  ##################################################################################################

  # All ducttape files will be written underneath this directory
  ducttape_output="out"

  encoder_type="transformer"
  decoder_type="transformer"

  num_embed="512:512"

  num_layers=(TestMode: no=(NumLayers: 6_6="6:6" 10_2="10:2") yes="1:1")

  transformer_model_size=512
  transformer_attention_heads=8
  transformer_feed_forward_num_hidden=2048

  train_batch_type=(TestMode: no="word" yes="sentence")
  train_batch_size=(TestMode: no="4096" yes=8)

  # Sockeye
  train_checkpoint_freq=(TestMode: no=5000 yes=100)
  train_max_checkpoints_not_improved=(TestMode: no=5 yes=0)

  # the number of sentences to use from the dev set
  train_num_decode_and_eval=(TestMode: no=500 yes=10)

  # TEST CONFIGURATIONS
  test_beam_size=(TestMode: no="12" yes="1")
  test_batch_size=5
  test_max_sent_length=100

  # SOURCE FACTORS
  # the factors to compute
  source_factors="bpe case"
  # how to combine factors: "concat" or "sum"
  source_factors_combine="sum"
  # set to same as num_embed if using combine method "sum"
  source_factors_num_embed="512 512"

  ##################################################################################################
  # Job submission parameters
  ##################################################################################################

  # SGE: flags for notifying about job completion (put in your email address!)
  action_flags="-m ae -M dings@jhu.edu"

  # SGE: generic job flags
  resource_flags="-l mem_free=2g"

  # SGE: larger job flags
  resource_flags_16g="-l hostname=c*,mem_free=16g"

  # SGE: flags for training a model
  num_devices=1
  resource_flags_train="-q g.q -l hostname=c*,gpu=1,mem_free=20g"

  # SGE: flags for decoding
  resource_flags_decode="-q g.q -l hostname=c*,gpu=1,mem_free=4g"

  # The default submitter: shell (run locally) or sge (run on a grid)
  submitter=(TestMode: no="sge" yes="shell")

  # Virtual env location. This should be a file path to the virtual env you want loaded before tasks.
  # This variable supports both conda and Python's virtualenv. For conda, use "conda:ENV" as the value,
  # where "ENV" is the name of the conda environment that should be loaded. For virtualenv, supply
  # the path to the script that should be loaded.
  # pyenv=(TestMode: no="conda:sockeye" yes="conda:sockeye-cpu")
  pyenv="/home/shuoyangd/pyenv/py3mx/bin/activate"

  ##################################################################################################
  # Preprocessing options
  ##################################################################################################

  # sentencepiece options
  sentencepiece_vocab_size=8000
  sentencepiece_model_type="unigram"

  # no of BPE operations
  bpe_operations=32000

  # options for cleaning training data
  MaxLen=100
  Ratio=1

  use_cpu=(TestMode: no yes)
}

plan test {
  reach multi_bleu, sacrebleu via
    (SubwordMethod: bpe) * (DoTokenize: yes) * (Casing: *) * (SourceFactors: yes) *
    (TestMode: yes)

  # reach sacrebleu via
  #   (SubwordMethod: bpe) * (DoTokenize: yes) * (Casing: actual) *
  #   (TestMode: yes)
}

plan transformer {
  reach sacrebleu via
    (SubwordMethod: bpe) * (DoTokenize: yes) * (Casing: actual) * (NumLayers: 6_6) * (Direction: *)
}
